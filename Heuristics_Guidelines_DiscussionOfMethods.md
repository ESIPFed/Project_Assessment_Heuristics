## Ranking Effectiveness (“Usefulness”) of Communication Vehicles (CVs)  

In Jakob Nielsen’s article, “Usability 101:  Introduction to Usability”, he discusses five quality components of usability which have been used as the categories for ranking the effectiveness of the communication vehicles that a research project, tool or technology uses to convey information about itself.  While Nielsen describes these components as being important for assessing the “usability” of a user interface, if they are applied in order to determine the overall “utility” (defined as the “functionality” associated with a CV) that is offered by all the CVs available, the components seem very appropriate for ranking the effectiveness/ usefulness of the CVs per the definition of the term as described above (utility + usability = usefulness).  
The five components can be better understood by asking the following questions:
*	Learnability:  How easy is it for users to accomplish basic tasks the first time they encounter the CVs?
*	Efficiency:   Once users have learned the design, how quickly can they answer the questions they have or the tasks that they wish to accomplish?
*	Memorability:  When users return to the CVs after a period of not using them, how easily can they establish proficiency, or (re)find what they are looking for?
*	Errors:  How many errors do users make that result from the CV, how severe are the errors, and how easily can users recover from the errors? (Note a discussion of “errors” in user interface design can be found at:  https://www.nngroup.com/articles/slips/ )?
*	Satisfaction:  How pleasant is it to use the CVs?  

Questions that are specific to the points of view represented by the experts defined in the use cases can be found on the Evaluator Checklist for each of these categories. 

## Development of Assessment Tool as Evaluator Checklist 

The attached assessment tool (in MS Excel format) for the evaluators was derived from the application of the concepts found to be pertinent in the usability literature to the technology infusion & adoption concepts in the TRL evaluation literature.  It has been built to accommodate the perspectives of the four experts described in the use cases, but, hopefully, is broad enough to be applicable to other use cases as well which may include non-experts.    

The Evaluator Checklist for the Research Domain Expert is the most comprehensive in terms of the specific questions that are included within the categories of “understanding”, “assessing” and “packaging” per the scope of the research.  Each of the checklists has similar questions at present, but could be much more tailored to the specific questions and/or tasks that each expert might have (as well as a non-expert).  In addition, further narrowing the use case to include one or more “personas” (defined as a fictitious identify that reflects one of the user groups for whom the CV is designed) might be productive.  As previously mentioned, the questions focus primarily upon CVs that may include project websites, documentation, lists of references, online help text and/or tutorials, testimonials, etc.  The questions could be used at different points in the evaluation process.  For example, the questions related to “understanding” the research project, tool or technology could and probably should be asked, answered and scored early in the evaluation process as it would be more difficult to accurately remember first impressions later in the evaluation process.    

The scoring for the questions in the five component categories was designed to be qualitative rather than quantitative in order to lessen the sense of “grading” a communication vehicle, and thereby implying failure.  Given the fact that this kind of evaluation is, by definition, subjective, the qualitative scoring (Very poor / Poor / Moderate / Good / Excellent) plus the opportunity for the evaluator to have ample room for comments about the scoring and/or suggestions was intended to convey constructive information that could be used to improve the CVs.   

## Feedback on draft Evaluator Checklist for the Domain Research Expert  

The Research Domain Expert Evaluator Checklist was initially presented for community feedback at the ESIP Winter 2017 meeting in Washington, DC in January of 2017 at a breakout session.  Comments after the presentation (attached as a PDF file) were as follows:
*	After generating questions from the ESIP community (and others, as feasible) for the other domain experts, it would be very useful to do some testing of this instrument on a research project or other kind of project with at least the four domain experts that are described within the use cases, if not groups of domain experts and then correlate the responses to see how the evaluation process categories and the ranking categories work with each of the groups.  Part of the purpose for this testing would be to see whether the matrix approach is too subjective to be useful.  
*	Would be useful to know more about how the different types of assessments work (i.e., a usability assessment in the classic case of user interface design, and a TRL assessment), and have more discussion about how the application of one to the other could be done.  
*	Would be useful to know more about how TRLs have been adopted and adapted by the Earth Cube community and other communities as well.  (See Olechowskie, et al, article referenced above which provides some information on this topic as well as other articles & references noted in the article).  
*	Once the instrument has been tested, more specific guidelines would be very helpful, with examples from actual websites or other communication vehicles.  
It is useful to note, that several people at the session were interested in using / adapting the evaluation heuristics for they found them potentially helpful to assess project web sites that they were planning to undertake, and/or training that they were planning to provide to early career scientists.   

## Draft Guidelines in support of the user interface usability assessment aspects of the Evaluator Checklist  

For those not familiar with the process of testing the usability of user interfaces to CVs such as those of research project web sites, it might be difficult to know what to look for when stepping back to evaluate what is seen on the web page in terms of its presentation of information rather than for its intellectual content.  For this reason, “Usability Guidelines” have been added to the attached Evaluation Checklist Excel file that are included as part of the Usability Review Template link in the article from UX for the Masses referenced above, called “A guide to carrying out usability reviews” (February 11, 2011).   The areas on a web site or other CV to which one using the Usability Guidelines is directed include its form and functionality, its homepage / starting page;  its navigation,  its search capabilities; its control and feedback, its forms, its errors;  its content and text; its help and its performance.  The Usability Guidelines include more specific questions within each of these areas and rates them by importance to the successful user experience.    

## Discussion
### Evaluation process:  

The questions that are listed on the Evaluator Checklist relate to the information that the domain expert could / would ask in order to achieve the evaluation of the research project, technology or product itself.  In order to actually do the scoring of the CVs according to the usability categories, however, it would be necessary for the evaluator to follow a three-step process for each question.  The first step would be to look at the intellectual content found on the CV that is available to answer the question while also noting where the information is found.  The second step of the process is to assess the features of the CV where the information is found, and score those features from Very poor – Excellent.  The third step would be to add comments that explain what aspects of the feature were particularly helpful, or could be improved.  The final step would be an important one for each evaluator to do in order to provide the most constructive information to the creators of the CVs.   

For those not familiar with user interface testing, it may be difficult to distinguish and identify the features and/or capabilities seen on a CV.  For this reason, the Usability Guidelines have been included on the worksheet that contains the Evaluator Checklists.  As mentioned above, the categories that are included on the Usability Guidelines include the features and functionality of the CV,  the  homepage or starting page, its navigation,  its search function, the means of control and feedback, its forms,  its content and text, the help available, and the overall performance.  The questions asked under each of these categories in the Usability Guidelines can help an evaluator stand back from the intellectual content of the CV to think about and evaluate whether and how the semantic information came through.    

For example, let’s say an evaluator is just beginning to look at NASA’s Hurricanes and Tropical Storms website at:  https://www.nasa.gov/mission_pages/hurricanes/main/index.html  in order to answer the following question in the Understanding part of the evaluation process:  “What is / are the research domain(s) with which this research is associated?”  Presumably, the first place to look for the information that will allow the evaluator to answer this question would be the home page.   The Usability Guidelines note that a feature of high importance for the homepage or starting page is whether it is “effective in orienting and directing users to their desired information and tasks.  Users should be able to work out where they need to go to complete a given task.”  In this case, the task would be to answer the question about what research domains are associated with this website.   On the homepage of this website, below the section of the home page that identifies it as being about “Hurricanes and Tropical Storms”, there is an arrow pointing to subtopics that include an “Overview”.  Clicking on the Overview tab takes the evaluator to another page that provides subtopics related to hurricanes with small images, a single sentence explanation and a link to subpages.   What is not immediately obvious is what or whether there are subtopics related to tropical storms and/or what the semantic relationship is between hurricanes and tropical storms.  Is one topic a subset of the other?  To a research domain expert, that may not be a question worth answering on this page of the website; to a member of the general public, it might be a relevant question.  In either case, it would be necessary to investigate further within the site to see what or whether there were more specific topics covered that relate to tropical storms.  If the evaluator were to backtrack to the home page, there is a section on the lefthand side that includes links to “Related Topics.”  Tropical Storms are not listed here either, so further navigation is necessary.  Given these features, how should / would an evaluator score the “learnability” of the website, i.e., how easy was it to learn the answer to the question about what research domains are associated with this website?  

Clearly, an evaluator could spend a great deal of time looking closely at all the features of a CV, and also answering the questions posed at each step of the evaluation process.  The amount of time allocated will have to be determined based on the role of the evaluator, and the importance of that role to the overall evaluation process.  What might be more feasible would be to have two stages of evaluation (or two different evaluators?), one to answer the technical questions related to the technology infusion or adoptability potential for a research project or product, and another to address the effectiveness of the CVs in communicating important information that would influence its technology infusion or adoptability potential.  



