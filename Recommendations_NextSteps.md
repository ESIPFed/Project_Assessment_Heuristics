As of February 2017, these were the recommendations / next steps suggested:  
*	Add to questions for 3 use cases besides the research domain expert
*	Add different “persona” approach to augment and clarify the Use Cases
*	Test in a couple of venues with web sites relating to each of the use cases, i.e., a research project, a tool site, a project describing a workflow (hard to find?), an AIST technology project, e.g., – e.g., with Stace Beaulieu, from BCO-DMO, and/or Natalie Meyers from the Open Science Framework  on research project web sites.  Test for feedback upon, for example:  
**	The “Usefulness” approach – melding of the two areas of research
**	The viability of the questions from each use case / persona POV
**	The utility of the rating mechanism (i.e., as subjective, but non-quantitative)
*	Iterate the questions on the Checklists based on the testing
*	Re-draft the guidelines based on the testing using a specific research web site as an example 
*	Perhaps write a paper for publication after testing in conjunction with the AIST assessment if these heuristics prove useful to those efforts
